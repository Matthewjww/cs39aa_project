{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"By Matthew White\n\nCitations: Matthew Wattson, https://keras.io/guides/keras_nlp/transformer_pretraining/\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-12-06T15:37:49.334909Z","iopub.execute_input":"2022-12-06T15:37:49.335339Z","iopub.status.idle":"2022-12-06T15:37:49.377810Z","shell.execute_reply.started":"2022-12-06T15:37:49.335236Z","shell.execute_reply":"2022-12-06T15:37:49.376504Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/nyt-news/nyt_all.csv\n/kaggle/input/nyt-news/nyt_val.csv\n/kaggle/input/nyt-news/nyt_train.csv\n/kaggle/input/nyt-news/nyt_test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install rouge_score","metadata":{"execution":{"iopub.status.busy":"2022-12-06T15:37:51.104847Z","iopub.execute_input":"2022-12-06T15:37:51.105273Z","iopub.status.idle":"2022-12-06T15:38:03.904051Z","shell.execute_reply.started":"2022-12-06T15:37:51.105238Z","shell.execute_reply":"2022-12-06T15:38:03.902881Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting rouge_score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: absl-py in /opt/conda/lib/python3.7/site-packages (from rouge_score) (0.15.0)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.7/site-packages (from rouge_score) (3.7)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from rouge_score) (1.21.6)\nRequirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.7/site-packages (from rouge_score) (1.15.0)\nRequirement already satisfied: regex>=2021.8.3 in /opt/conda/lib/python3.7/site-packages (from nltk->rouge_score) (2021.11.10)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from nltk->rouge_score) (4.64.0)\nRequirement already satisfied: joblib in /opt/conda/lib/python3.7/site-packages (from nltk->rouge_score) (1.0.1)\nRequirement already satisfied: click in /opt/conda/lib/python3.7/site-packages (from nltk->rouge_score) (8.0.4)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from click->nltk->rouge_score) (4.13.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click->nltk->rouge_score) (3.8.0)\nRequirement already satisfied: typing-extensions>=3.6.4 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->click->nltk->rouge_score) (4.1.1)\nBuilding wheels for collected packages: rouge_score\n  Building wheel for rouge_score (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24955 sha256=94f7c89b6a0addd79fa46a43fd5ae911911a8c454a7876159c3b84bd082f3f6a\n  Stored in directory: /root/.cache/pip/wheels/84/ac/6b/38096e3c5bf1dc87911e3585875e21a3ac610348e740409c76\nSuccessfully built rouge_score\nInstalling collected packages: rouge_score\nSuccessfully installed rouge_score-0.1.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"!pip install keras-nlp","metadata":{"execution":{"iopub.status.busy":"2022-12-06T15:38:06.173511Z","iopub.execute_input":"2022-12-06T15:38:06.174189Z","iopub.status.idle":"2022-12-06T15:42:12.419954Z","shell.execute_reply.started":"2022-12-06T15:38:06.174146Z","shell.execute_reply":"2022-12-06T15:42:12.418719Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting keras-nlp\n  Downloading keras_nlp-0.3.1-py3-none-any.whl (151 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.1/151.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from keras-nlp) (1.21.6)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from keras-nlp) (21.3)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.7/site-packages (from keras-nlp) (0.15.0)\nCollecting tensorflow-text\n  Downloading tensorflow_text-2.11.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m53.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: tensorflow in /opt/conda/lib/python3.7/site-packages (from keras-nlp) (2.6.4)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from absl-py->keras-nlp) (1.15.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->keras-nlp) (3.0.9)\nCollecting tensorboard<2.7,>=2.6.0\n  Downloading tensorboard-2.6.0-py3-none-any.whl (5.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: tensorflow-estimator<2.7,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow->keras-nlp) (2.6.0)\nCollecting typing-extensions<3.11,>=3.7\n  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\nRequirement already satisfied: astunparse~=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorflow->keras-nlp) (1.6.3)\nRequirement already satisfied: gast==0.4.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow->keras-nlp) (0.4.0)\nRequirement already satisfied: termcolor~=1.1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow->keras-nlp) (1.1.0)\nRequirement already satisfied: google-pasta~=0.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow->keras-nlp) (0.2.0)\nRequirement already satisfied: grpcio<2.0,>=1.37.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow->keras-nlp) (1.43.0)\nRequirement already satisfied: keras-preprocessing~=1.1.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow->keras-nlp) (1.1.2)\nRequirement already satisfied: wrapt~=1.12.1 in /opt/conda/lib/python3.7/site-packages (from tensorflow->keras-nlp) (1.12.1)\nRequirement already satisfied: wheel~=0.35 in /opt/conda/lib/python3.7/site-packages (from tensorflow->keras-nlp) (0.37.1)\nRequirement already satisfied: flatbuffers~=1.12.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow->keras-nlp) (1.12)\nRequirement already satisfied: clang~=5.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow->keras-nlp) (5.0)\nRequirement already satisfied: keras<2.7,>=2.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow->keras-nlp) (2.6.0)\nRequirement already satisfied: opt-einsum~=3.3.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow->keras-nlp) (3.3.0)\nCollecting numpy\n  Downloading numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.8/14.8 MB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting h5py~=3.1.0\n  Downloading h5py-3.1.0-cp37-cp37m-manylinux1_x86_64.whl (4.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: protobuf>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow->keras-nlp) (3.19.4)\nCollecting tensorflow\n  Downloading tensorflow-2.11.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: tensorflow-hub>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-text->keras-nlp) (0.12.0)\nINFO: pip is looking at multiple versions of tensorflow-text to determine which version is compatible with other requirements. This could take a while.\nCollecting tensorflow-text\n  Downloading tensorflow_text-2.10.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting tensorflow\n  Downloading tensorflow-2.10.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (578.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m578.1/578.1 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Downloading tensorflow-2.10.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (578.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m578.0/578.0 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting tensorflow-text\n  Downloading tensorflow_text-2.9.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow->keras-nlp) (0.4.6)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow->keras-nlp) (1.8.1)\nRequirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow->keras-nlp) (0.6.1)\nRequirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow->keras-nlp) (1.35.0)\nRequirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow->keras-nlp) (2.2.2)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow->keras-nlp) (59.8.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow->keras-nlp) (3.3.7)\nRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard<2.7,>=2.6.0->tensorflow->keras-nlp) (2.28.1)\nCollecting tensorflow\n  Downloading tensorflow-2.9.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.8/511.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Downloading tensorflow-2.9.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.8/511.8 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Downloading tensorflow-2.9.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.7/511.7 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Downloading tensorflow-2.9.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (511.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m511.7/511.7 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting tensorflow-text\n  Downloading tensorflow_text-2.8.2-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hCollecting tensorflow\n  Downloading tensorflow-2.8.4-cp37-cp37m-manylinux2010_x86_64.whl (497.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m497.9/497.9 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Downloading tensorflow-2.8.3-cp37-cp37m-manylinux2010_x86_64.whl (497.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m497.9/497.9 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Downloading tensorflow-2.8.2-cp37-cp37m-manylinux2010_x86_64.whl (497.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m497.9/497.9 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Downloading tensorflow-2.8.1-cp37-cp37m-manylinux2010_x86_64.whl (497.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m497.9/497.9 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Downloading tensorflow-2.8.0-cp37-cp37m-manylinux2010_x86_64.whl (497.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m497.5/497.5 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting tensorflow-text\n  Downloading tensorflow_text-2.8.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (4.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h  Downloading tensorflow_text-2.7.3-cp37-cp37m-manylinux2010_x86_64.whl (4.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting tensorflow\n  Downloading tensorflow-2.7.4-cp37-cp37m-manylinux2010_x86_64.whl (495.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m495.5/495.5 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting tensorflow-io-gcs-filesystem>=0.21.0\n  Downloading tensorflow_io_gcs_filesystem-0.28.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hCollecting libclang>=9.0.1\n  Downloading libclang-14.0.6-py2.py3-none-manylinux2010_x86_64.whl (14.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting tensorflow\n  Downloading tensorflow-2.7.3-cp37-cp37m-manylinux2010_x86_64.whl (495.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m495.4/495.4 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Downloading tensorflow-2.7.2-cp37-cp37m-manylinux2010_x86_64.whl (495.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m495.4/495.4 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Downloading tensorflow-2.7.1-cp37-cp37m-manylinux2010_x86_64.whl (495.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m495.0/495.0 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Downloading tensorflow-2.7.0-cp37-cp37m-manylinux2010_x86_64.whl (489.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m489.6/489.6 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting tensorflow-text\n  Downloading tensorflow_text-2.7.0-cp37-cp37m-manylinux2010_x86_64.whl (4.9 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h  Downloading tensorflow_text-2.6.0-cp37-cp37m-manylinux1_x86_64.whl (4.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m81.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: cached-property in /opt/conda/lib/python3.7/site-packages (from h5py~=3.1.0->tensorflow->keras-nlp) (1.5.2)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow->keras-nlp) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow->keras-nlp) (0.2.7)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow->keras-nlp) (4.8)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow->keras-nlp) (1.3.1)\nRequirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow->keras-nlp) (4.13.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow->keras-nlp) (1.26.12)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow->keras-nlp) (2.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow->keras-nlp) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.7,>=2.6.0->tensorflow->keras-nlp) (2022.9.24)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.7/site-packages (from werkzeug>=0.11.15->tensorboard<2.7,>=2.6.0->tensorflow->keras-nlp) (2.1.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.7,>=2.6.0->tensorflow->keras-nlp) (3.8.0)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.7,>=2.6.0->tensorflow->keras-nlp) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.7,>=2.6.0->tensorflow->keras-nlp) (3.2.0)\nInstalling collected packages: typing-extensions, numpy, h5py, tensorboard, tensorflow-text, keras-nlp\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing_extensions 4.1.1\n    Uninstalling typing_extensions-4.1.1:\n      Successfully uninstalled typing_extensions-4.1.1\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.21.6\n    Uninstalling numpy-1.21.6:\n      Successfully uninstalled numpy-1.21.6\n  Attempting uninstall: h5py\n    Found existing installation: h5py 3.7.0\n    Uninstalling h5py-3.7.0:\n      Successfully uninstalled h5py-3.7.0\n  Attempting uninstall: tensorboard\n    Found existing installation: tensorboard 2.10.1\n    Uninstalling tensorboard-2.10.1:\n      Successfully uninstalled tensorboard-2.10.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-io 0.21.0 requires tensorflow-io-gcs-filesystem==0.21.0, which is not installed.\ndask-cudf 21.10.1 requires cupy-cuda114, which is not installed.\nbeatrix-jupyterlab 3.1.7 requires google-cloud-bigquery-storage, which is not installed.\nxarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\ntfx-bsl 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<3,>=1.15.5, but you have tensorflow 2.6.4 which is incompatible.\ntensorflow-transform 1.9.0 requires tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,<2.10,>=1.15.5, but you have tensorflow 2.6.4 which is incompatible.\ntensorflow-serving-api 2.9.0 requires tensorflow<3,>=2.9.0, but you have tensorflow 2.6.4 which is incompatible.\nrich 12.6.0 requires typing-extensions<5.0,>=4.0.0; python_version < \"3.9\", but you have typing-extensions 3.10.0.2 which is incompatible.\npytorch-lightning 1.7.7 requires tensorboard>=2.9.1, but you have tensorboard 2.6.0 which is incompatible.\npytorch-lightning 1.7.7 requires typing-extensions>=4.0.0, but you have typing-extensions 3.10.0.2 which is incompatible.\npytools 2022.1.12 requires typing-extensions>=4.0; python_version < \"3.11\", but you have typing-extensions 3.10.0.2 which is incompatible.\npdpbox 0.2.1 requires matplotlib==3.1.1, but you have matplotlib 3.5.3 which is incompatible.\npandas-profiling 3.1.0 requires markupsafe~=2.0.1, but you have markupsafe 2.1.1 which is incompatible.\nnnabla 1.31.0 requires numpy>=1.20.0, but you have numpy 1.19.5 which is incompatible.\njaxlib 0.3.22+cuda11.cudnn805 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\njax 0.3.23 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\nflax 0.6.1 requires typing-extensions>=4.1.1, but you have typing-extensions 3.10.0.2 which is incompatible.\nflake8 4.0.1 requires importlib-metadata<4.3; python_version < \"3.8\", but you have importlib-metadata 4.13.0 which is incompatible.\nfeaturetools 1.11.1 requires numpy>=1.21.0, but you have numpy 1.19.5 which is incompatible.\ndask-cudf 21.10.1 requires dask==2021.09.1, but you have dask 2022.2.0 which is incompatible.\ndask-cudf 21.10.1 requires distributed==2021.09.1, but you have distributed 2022.2.0 which is incompatible.\ncmdstanpy 1.0.7 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\napache-beam 2.40.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.5.1 which is incompatible.\nallennlp 2.10.1 requires h5py>=3.6.0, but you have h5py 3.1.0 which is incompatible.\nallennlp 2.10.1 requires numpy>=1.21.4, but you have numpy 1.19.5 which is incompatible.\naioitertools 0.11.0 requires typing_extensions>=4.0; python_version < \"3.10\", but you have typing-extensions 3.10.0.2 which is incompatible.\naiobotocore 2.4.0 requires botocore<1.27.60,>=1.27.59, but you have botocore 1.27.93 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed h5py-3.1.0 keras-nlp-0.3.1 numpy-1.19.5 tensorboard-2.6.0 tensorflow-text-2.6.0 typing-extensions-3.10.0.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport logging\n\nimport nltk\nimport numpy as np\nimport keras_nlp\nimport tensorflow as tf\nfrom tensorflow import keras\n\n# Only log error messages\ntf.get_logger().setLevel(logging.ERROR)\n\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","metadata":{"execution":{"iopub.status.busy":"2022-12-06T15:44:52.832675Z","iopub.execute_input":"2022-12-06T15:44:52.833095Z","iopub.status.idle":"2022-12-06T15:44:58.828305Z","shell.execute_reply.started":"2022-12-06T15:44:52.833056Z","shell.execute_reply":"2022-12-06T15:44:58.827333Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"Establish some starting parameters where we can come back and change them easily to try different hyperparameters.","metadata":{}},{"cell_type":"code","source":"# The percentage of the dataset you want to split as train and test\nTRAIN_TEST_SPLIT = 0.1\n\nMAX_INPUT_LENGTH = 1024  # Maximum length of the input to the model\nMIN_TARGET_LENGTH = 5  # Minimum length of the output by the model\nMAX_TARGET_LENGTH = 128  # Maximum length of the output by the model\nBATCH_SIZE = 8  # Batch-size for training our model\nLEARNING_RATE = 2e-5  # Learning-rate for training our model\nMAX_EPOCHS = 1  # Maximum number of epochs we will train the model for\n\n# This notebook is built on the t5-small checkpoint from the Hugging Face Model Hub\nMODEL_CHECKPOINT = \"t5-small\"","metadata":{"execution":{"iopub.status.busy":"2022-12-06T12:24:52.963700Z","iopub.execute_input":"2022-12-06T12:24:52.964331Z","iopub.status.idle":"2022-12-06T12:24:52.974878Z","shell.execute_reply.started":"2022-12-06T12:24:52.964295Z","shell.execute_reply":"2022-12-06T12:24:52.972917Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"Loading our data set into train, validation, and test","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\n\nraw_datasets = load_dataset('csv', data_files={'train':'/kaggle/input/nyt-news/nyt_train.csv','validation':'/kaggle/input/nyt-news/nyt_val.csv', 'test':'/kaggle/input/nyt-news/nyt_test.csv'},delimiter='~')","metadata":{"execution":{"iopub.status.busy":"2022-12-06T12:24:52.979329Z","iopub.execute_input":"2022-12-06T12:24:52.979755Z","iopub.status.idle":"2022-12-06T12:24:54.714045Z","shell.execute_reply.started":"2022-12-06T12:24:52.979711Z","shell.execute_reply":"2022-12-06T12:24:54.713053Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Downloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-a8f2af29a5ec2f9a/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97c072be3acd4a5e9b3351e6d1abd17a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1476b5b445f64e619a454645f61d90a4"}},"metadata":{}},{"name":"stdout","text":"Dataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-a8f2af29a5ec2f9a/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9696999713a4538babea516c74f378b"}},"metadata":{}}]},{"cell_type":"code","source":"print(raw_datasets)","metadata":{"execution":{"iopub.status.busy":"2022-12-06T12:24:54.715495Z","iopub.execute_input":"2022-12-06T12:24:54.716443Z","iopub.status.idle":"2022-12-06T12:24:54.722871Z","shell.execute_reply.started":"2022-12-06T12:24:54.716404Z","shell.execute_reply":"2022-12-06T12:24:54.721933Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['Unnamed: 0', 'title', 'maintext'],\n        num_rows: 6000\n    })\n    validation: Dataset({\n        features: ['Unnamed: 0', 'title', 'maintext'],\n        num_rows: 3000\n    })\n    test: Dataset({\n        features: ['Unnamed: 0', 'title', 'maintext'],\n        num_rows: 1000\n    })\n})\n","output_type":"stream"}]},{"cell_type":"code","source":"print(raw_datasets['train'][0])","metadata":{"execution":{"iopub.status.busy":"2022-12-06T12:24:54.724408Z","iopub.execute_input":"2022-12-06T12:24:54.725072Z","iopub.status.idle":"2022-12-06T12:24:54.734215Z","shell.execute_reply.started":"2022-12-06T12:24:54.725034Z","shell.execute_reply":"2022-12-06T12:24:54.733187Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"{'Unnamed: 0': 0, 'title': 'On Sundays, Foursquare Co-founder Goes Online, Then Out for a Walk', 'maintext': 'NIBBLE OF NEW YORK I don’t have a set schedule. But we go on these long walks. We just kind of snake our way through the Lower East Side, parts of the East Village, through SoHo, come up through Chelsea. And we bring people with us, and it’s a way to introduce people to new things. It’s about trying to do as much as we can in one day, but in bite-sized increments. Like we’ll stop at La Esquina and get a horchata. You’ll walk down Orchard Street and stop at four different galleries. We’ll go to Hester Street and each have one dumpling. You might have seven appetizers over the course of the day and stop and get two or three things to drink. And get to check out a little art on the way, and just try to find things we haven’t found before. PATH AND DEVIATIONS Sometimes we’ll do that for five or six hours. The path is always similar: walk down Avenue B, end up on Clinton Street, walk down Stanton near Schiller’s to Orchard. We have a general path because even on a boring day you know that there’s 10 art galleries, three staple go-to food places. And the more we can deviate from the path, the more fun it is. When it’s nice, the perfect end to a walk is Zum Schneider for a beer and a pretzel outside.'}\n","output_type":"stream"}]},{"cell_type":"code","source":"#raw_datasets = raw_datasets.train_test_split(\n#train_size=TRAIN_TEST_SPLIT, test_size=TRAIN_TEST_SPLIT\n#)","metadata":{"execution":{"iopub.status.busy":"2022-12-06T12:24:54.735906Z","iopub.execute_input":"2022-12-06T12:24:54.736586Z","iopub.status.idle":"2022-12-06T12:24:55.056650Z","shell.execute_reply.started":"2022-12-06T12:24:54.736549Z","shell.execute_reply":"2022-12-06T12:24:55.054022Z"},"trusted":true},"execution_count":9,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/3556913525.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m raw_datasets = raw_datasets.train_test_split(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtrain_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTRAIN_TEST_SPLIT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTRAIN_TEST_SPLIT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m )\n","\u001b[0;31mAttributeError\u001b[0m: 'DatasetDict' object has no attribute 'train_test_split'"],"ename":"AttributeError","evalue":"'DatasetDict' object has no attribute 'train_test_split'","output_type":"error"}]},{"cell_type":"markdown","source":"tokenizing the dataset with the tokenizer from the model we will be re-training","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT, model_max_length=512)","metadata":{"execution":{"iopub.status.busy":"2022-12-06T12:27:25.519761Z","iopub.execute_input":"2022-12-06T12:27:25.520156Z","iopub.status.idle":"2022-12-06T12:27:30.859165Z","shell.execute_reply.started":"2022-12-06T12:27:25.520122Z","shell.execute_reply":"2022-12-06T12:27:30.858191Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9ed445bb8cf4feb8fa9ff59cf917082"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/773k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1eca8955fccd4fffbb849a85db8dc83d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.32M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ebf459b7ad514d8aa9373ab1ae1f053e"}},"metadata":{}}]},{"cell_type":"code","source":"if MODEL_CHECKPOINT in [\"t5-small\", \"t5-base\", \"t5-large\", \"t5-3b\", \"t5-11b\"]:\n    prefix = \"summarize: \"\nelse:\n    prefix = \"\"","metadata":{"execution":{"iopub.status.busy":"2022-12-06T12:27:44.354093Z","iopub.execute_input":"2022-12-06T12:27:44.355224Z","iopub.status.idle":"2022-12-06T12:27:44.361024Z","shell.execute_reply.started":"2022-12-06T12:27:44.355172Z","shell.execute_reply":"2022-12-06T12:27:44.359996Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"def preprocess_function(examples):\n    inputs = [prefix + doc for doc in examples[\"maintext\"]]\n    model_inputs = tokenizer(inputs, max_length=MAX_INPUT_LENGTH, truncation=True)\n\n    # Setup the tokenizer for targets\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(\n            examples[\"title\"], max_length=MAX_TARGET_LENGTH, truncation=True\n        )\n\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n\n    return model_inputs","metadata":{"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)","metadata":{"execution":{"iopub.status.busy":"2022-12-06T12:27:49.723184Z","iopub.execute_input":"2022-12-06T12:27:49.723544Z","iopub.status.idle":"2022-12-06T12:28:06.380823Z","shell.execute_reply.started":"2022-12-06T12:27:49.723511Z","shell.execute_reply":"2022-12-06T12:28:06.376693Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/6 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10bef08241e747069e9015cb58e9fd61"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a89ceb7b6414183911dfa1d0a753bbc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed85074e349941998c6901ed3fecac72"}},"metadata":{}}]},{"cell_type":"markdown","source":" Here we define our 'model'. We are using a model that already exists so we are really just creating that then setting up the training parameters so we can re-train it.","metadata":{}},{"cell_type":"code","source":"from transformers import TFAutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n\nmodel = TFAutoModelForSeq2SeqLM.from_pretrained(MODEL_CHECKPOINT)","metadata":{"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"2022-12-06 12:28:11.137370: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-06 12:28:11.138584: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-06 12:28:11.139320: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-06 12:28:11.141452: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\nTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n2022-12-06 12:28:11.141804: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-06 12:28:11.142513: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-06 12:28:11.143252: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-06 12:28:15.895759: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-06 12:28:15.896591: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-06 12:28:15.897288: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n2022-12-06 12:28:15.897907: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15043 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/231M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72fca62b3777423d9dbc67a32d77ceba"}},"metadata":{}},{"name":"stderr","text":"2022-12-06 12:28:30.853988: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n2022-12-06 12:28:32.475298: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 65798144 exceeds 10% of free system memory.\nAll model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n\nAll the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import DataCollatorForSeq2Seq\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"tf\")","metadata":{"execution":{"iopub.status.busy":"2022-12-06T12:28:40.087417Z","iopub.execute_input":"2022-12-06T12:28:40.087797Z","iopub.status.idle":"2022-12-06T12:28:40.093271Z","shell.execute_reply.started":"2022-12-06T12:28:40.087765Z","shell.execute_reply":"2022-12-06T12:28:40.092154Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n    batch_size=BATCH_SIZE,\n    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n    shuffle=True,\n    collate_fn=data_collator,\n)\ntest_dataset = tokenized_datasets[\"test\"].to_tf_dataset(\n    batch_size=BATCH_SIZE,\n    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n    shuffle=False,\n    collate_fn=data_collator,\n)\ngeneration_dataset = (\n    tokenized_datasets[\"test\"]\n    .shuffle()\n    .select(list(range(200)))\n    .to_tf_dataset(\n        batch_size=BATCH_SIZE,\n        columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n        shuffle=False,\n        collate_fn=data_collator,\n    )\n)","metadata":{"execution":{"iopub.status.busy":"2022-12-06T12:28:44.361773Z","iopub.execute_input":"2022-12-06T12:28:44.362162Z","iopub.status.idle":"2022-12-06T12:28:46.101263Z","shell.execute_reply.started":"2022-12-06T12:28:44.362129Z","shell.execute_reply":"2022-12-06T12:28:46.100373Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\nmodel.compile(optimizer=optimizer)","metadata":{"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n","output_type":"stream"}]},{"cell_type":"code","source":"import keras_nlp\n\nrouge_l = keras_nlp.metrics.RougeL()\n\n\ndef metric_fn(eval_predictions):\n    predictions, labels = eval_predictions\n    decoded_predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    for label in labels:\n        label[label < 0] = tokenizer.pad_token_id  # Replace masked label tokens\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    result = rouge_l(decoded_labels, decoded_predictions)\n    # We will print only the F1 score, you can use other aggregation metrics as well\n    result = {\"RougeL\": result[\"f1_score\"]}\n\n    return result","metadata":{"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"Finally we fit the t5 model to our data set","metadata":{}},{"cell_type":"code","source":"from transformers.keras_callbacks import KerasMetricCallback\n\nmetric_callback = KerasMetricCallback(\n    metric_fn, eval_dataset=generation_dataset, predict_with_generate=True\n)\n\ncallbacks = [metric_callback]\n\n# For now we will use our test set as our validation_data\nmodel.fit(\n    train_dataset, validation_data=test_dataset, epochs=MAX_EPOCHS, callbacks=callbacks\n)","metadata":{"execution":{"iopub.status.busy":"2022-12-06T12:29:01.713538Z","iopub.execute_input":"2022-12-06T12:29:01.714240Z","iopub.status.idle":"2022-12-06T12:34:42.494700Z","shell.execute_reply.started":"2022-12-06T12:29:01.714202Z","shell.execute_reply":"2022-12-06T12:34:42.493729Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"2022-12-06 12:29:01.788111: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n","output_type":"stream"},{"name":"stdout","text":"750/750 [==============================] - 286s 356ms/step - loss: 3.9060 - val_loss: 3.2657\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7f9020673510>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Example 1:","metadata":{}},{"cell_type":"code","source":"print(raw_datasets[\"test\"][0][\"maintext\"])","metadata":{"execution":{"iopub.status.busy":"2022-12-06T12:42:06.437959Z","iopub.execute_input":"2022-12-06T12:42:06.438338Z","iopub.status.idle":"2022-12-06T12:42:06.444960Z","shell.execute_reply.started":"2022-12-06T12:42:06.438304Z","shell.execute_reply":"2022-12-06T12:42:06.443804Z"},"trusted":true},"execution_count":28,"outputs":[{"name":"stdout","text":"A Bronx teenager who the police say stabbed his mother three times in the chest was charged with murder yesterday. The man, Brandon Elliott, 19, was charged in the attack, which occurred Tuesday on East 224th Street in the Williamsbridge section. He lived with his mother, Bridget Johnson, 42. She was taken to Our Lady of Mercy Medical Center, where she died last night, the police said. Tina Kelley (NYT)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Generated title from model","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline\n\nsummarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer, framework=\"tf\")\n\nsummarizer(\n    raw_datasets[\"test\"][0][\"maintext\"],\n    min_length=MIN_TARGET_LENGTH,\n    max_length=MAX_TARGET_LENGTH,\n)","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:15:10.266969Z","iopub.execute_input":"2022-12-06T16:15:10.267343Z","iopub.status.idle":"2022-12-06T16:16:02.143548Z","shell.execute_reply.started":"2022-12-06T16:15:10.267313Z","shell.execute_reply":"2022-12-06T16:16:02.142419Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stderr","text":"Your max_length is set to 128, but you input_length is only 97. You might consider decreasing max_length manually, e.g. summarizer('...', max_length=48)\n2022-12-06 16:15:10.742308: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 65798144 exceeds 10% of free system memory.\n2022-12-06 16:15:11.169733: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 65798144 exceeds 10% of free system memory.\n2022-12-06 16:15:11.579521: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 65798144 exceeds 10% of free system memory.\n","output_type":"stream"},{"execution_count":42,"output_type":"execute_result","data":{"text/plain":"[{'summary_text': 'Brandon Elliott, 19, stabbed his mother three times in the chest .'}]"},"metadata":{}}]},{"cell_type":"markdown","source":"Actual title","metadata":{}},{"cell_type":"code","source":"print(raw_datasets[\"test\"][0][\"title\"])","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:16:12.042126Z","iopub.execute_input":"2022-12-06T16:16:12.043161Z","iopub.status.idle":"2022-12-06T16:16:12.048870Z","shell.execute_reply.started":"2022-12-06T16:16:12.043120Z","shell.execute_reply":"2022-12-06T16:16:12.047683Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"Metro Briefing | New York: Bronx: Teenager Charged In Murder\n","output_type":"stream"}]},{"cell_type":"markdown","source":"!pip install evaluate","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:14:32.770473Z","iopub.execute_input":"2022-12-06T16:14:32.770830Z","iopub.status.idle":"2022-12-06T16:14:42.947682Z","shell.execute_reply.started":"2022-12-06T16:14:32.770801Z","shell.execute_reply":"2022-12-06T16:14:42.946232Z"}}},{"cell_type":"markdown","source":"Here is the rouge score of our model generated title!","metadata":{}},{"cell_type":"code","source":"import evaluate\nrouge = evaluate.load('rouge')\npredictions = [\"Brandon Elliott, 19, stabbed his mother three times in the chest .\"]\nreferences = [\"A Bronx teenager who the police say stabbed his mother three times in the chest was charged with murder yesterday. The man, Brandon Elliott, 19, was charged in the attack, which occurred Tuesday on East 224th Street in the Williamsbridge section. He lived with his mother, Bridget Johnson, 42. She was taken to Our Lady of Mercy Medical Center, where she died last night, the police said. Tina Kelley (NYT)\"]\nresults = rouge.compute(predictions=predictions,references=references)\nprint(results)","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:36:05.826787Z","iopub.execute_input":"2022-12-06T16:36:05.827172Z","iopub.status.idle":"2022-12-06T16:36:06.208930Z","shell.execute_reply.started":"2022-12-06T16:36:05.827141Z","shell.execute_reply":"2022-12-06T16:36:06.207903Z"},"trusted":true},"execution_count":55,"outputs":[{"name":"stdout","text":"{'rouge1': 0.271604938271605, 'rouge2': 0.22784810126582278, 'rougeL': 0.19753086419753085, 'rougeLsum': 0.19753086419753085}\n","output_type":"stream"}]},{"cell_type":"code","source":"def evaluate_baseline(maintext,title, metric):\n    summaries = summarizer(maintext)\n    return metric.compute(predictions=summaries, references=title)","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:29:41.163221Z","iopub.execute_input":"2022-12-06T16:29:41.163629Z","iopub.status.idle":"2022-12-06T16:29:41.169089Z","shell.execute_reply.started":"2022-12-06T16:29:41.163593Z","shell.execute_reply":"2022-12-06T16:29:41.168061Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":" # Example 2:","metadata":{}},{"cell_type":"code","source":"print(raw_datasets[\"test\"][1][\"maintext\"])","metadata":{"execution":{"iopub.status.busy":"2022-12-06T13:41:07.259687Z","iopub.execute_input":"2022-12-06T13:41:07.260048Z","iopub.status.idle":"2022-12-06T13:41:07.266721Z","shell.execute_reply.started":"2022-12-06T13:41:07.260014Z","shell.execute_reply":"2022-12-06T13:41:07.265776Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"SAN ANTONIO, Texas — When the lights went out Monday night in the Alazán-Apache housing project in San Antonio — which stands in one of the city’s poorest ZIP codes — the traffic signals in the neighborhood flickered off and storekeepers pulled down their shutters. For residents, there was little left to do but huddle under blankets and hope that their children wouldn’t fall ill. “I need to take my kids somewhere to keep them warm. I don’t know where,” said Ricardo Cruz, 42, who lives at the Alazán-Apache Courts with his wife and five children, between 5 and 13 years old, and who has been without electricity since 7 p.m. Monday. While the rolling blackouts in Texas have left some 4 million residents without power in brutally cold weather, experts and community groups say that many marginalized communities were the first to be hit with power outages, and if history serves as a guide, could be among the last to be reconnected. This is particularly perilous, they say, given that low-income households can lack the financial resources to flee to safety or to rebound after the disruption.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Generated title from model","metadata":{}},{"cell_type":"code","source":"from transformers import pipeline\n\nsummarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer, framework=\"tf\")\n\nsummarizer(\n    raw_datasets[\"test\"][1][\"maintext\"],\n    min_length=MIN_TARGET_LENGTH,\n    max_length=MAX_TARGET_LENGTH,\n)","metadata":{"execution":{"iopub.status.busy":"2022-12-06T13:41:12.973592Z","iopub.execute_input":"2022-12-06T13:41:12.974337Z","iopub.status.idle":"2022-12-06T13:42:07.957579Z","shell.execute_reply.started":"2022-12-06T13:41:12.974298Z","shell.execute_reply":"2022-12-06T13:42:07.956588Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"[{'summary_text': 'Alazán-Apache Housing Project in San Antonio'}]"},"metadata":{}}]},{"cell_type":"markdown","source":"Actual title","metadata":{}},{"cell_type":"code","source":"print(raw_datasets[\"test\"][1][\"title\"])","metadata":{"execution":{"iopub.status.busy":"2022-12-06T13:42:57.823112Z","iopub.execute_input":"2022-12-06T13:42:57.823475Z","iopub.status.idle":"2022-12-06T13:42:57.831963Z","shell.execute_reply.started":"2022-12-06T13:42:57.823443Z","shell.execute_reply":"2022-12-06T13:42:57.830683Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"Texas Blackouts Hit Minority Neighborhoods Especially Hard\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Here is the rouge score of our model generated title!","metadata":{}},{"cell_type":"code","source":"import evaluate\nrouge = evaluate.load('rouge')\npredictions = [\"Alazán-Apache Housing Project in San Antonio\"]\nreferences = [\"SAN ANTONIO, Texas — When the lights went out Monday night in the Alazán-Apache housing project in San Antonio — which stands in one of the city’s poorest ZIP codes — the traffic signals in the neighborhood flickered off and storekeepers pulled down their shutters. For residents, there was little left to do but huddle under blankets and hope that their children wouldn’t fall ill. “I need to take my kids somewhere to keep them warm. I don’t know where,” said Ricardo Cruz, 42, who lives at the Alazán-Apache Courts with his wife and five children, between 5 and 13 years old, and who has been without electricity since 7 p.m. Monday. While the rolling blackouts in Texas have left some 4 million residents without power in brutally cold weather, experts and community groups say that many marginalized communities were the first to be hit with power outages, and if history serves as a guide, could be among the last to be reconnected. This is particularly perilous, they say, given that low-income households can lack the financial resources to flee to safety or to rebound after the disruption.\"]\nresults = rouge.compute(predictions=predictions,references=references)\nprint(results)","metadata":{"execution":{"iopub.status.busy":"2022-12-06T16:36:29.110732Z","iopub.execute_input":"2022-12-06T16:36:29.111110Z","iopub.status.idle":"2022-12-06T16:36:29.481598Z","shell.execute_reply.started":"2022-12-06T16:36:29.111077Z","shell.execute_reply":"2022-12-06T16:36:29.480524Z"},"trusted":true},"execution_count":56,"outputs":[{"name":"stdout","text":"{'rouge1': 0.0792079207920792, 'rouge2': 0.07, 'rougeL': 0.0792079207920792, 'rougeLsum': 0.0792079207920792}\n","output_type":"stream"}]},{"cell_type":"markdown","source":"**Trying to push this model to hugging face but it is not accepting my authorization. Doesn't matter too much, not part of the assignment.**","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2022-12-06T12:35:58.424468Z","iopub.execute_input":"2022-12-06T12:35:58.424845Z","iopub.status.idle":"2022-12-06T12:35:58.469000Z","shell.execute_reply.started":"2022-12-06T12:35:58.424812Z","shell.execute_reply":"2022-12-06T12:35:58.467976Z"},"trusted":true},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c638155a44d4586b138f60d22ba31ba"}},"metadata":{}}]},{"cell_type":"code","source":"model.push_to_hub(\"t5-small_nytnews\")\ntokenizer.push_to_hub(\"t5-small_nytnews\")","metadata":{"execution":{"iopub.status.busy":"2022-12-06T12:40:29.885627Z","iopub.execute_input":"2022-12-06T12:40:29.886054Z","iopub.status.idle":"2022-12-06T12:40:30.950056Z","shell.execute_reply.started":"2022-12-06T12:40:29.886020Z","shell.execute_reply":"2022-12-06T12:40:30.948582Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":26,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/huggingface_hub/utils/_errors.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1020\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/api/repos/create","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/4139955028.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_to_hub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"t5_nytnews_1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_to_hub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"t5_nytnews_1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mpush_to_hub\u001b[0;34m(self, repo_path_or_name, repo_url, use_temp_dir, commit_message, organization, private, use_auth_token, **model_card_kwargs)\u001b[0m\n\u001b[1;32m    934\u001b[0m             \u001b[0morganization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morganization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m             \u001b[0mprivate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprivate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 936\u001b[0;31m             \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    937\u001b[0m         )\n\u001b[1;32m    938\u001b[0m         \u001b[0;31m# Save the files in the cloned repo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36m_create_or_get_repo\u001b[0;34m(cls, repo_path_or_name, repo_url, organization, private, use_auth_token)\u001b[0m\n\u001b[1;32m   1006\u001b[0m             \u001b[0mrepo_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepo_path_or_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m             repo_url = cls._get_repo_url_from_name(\n\u001b[0;32m-> 1008\u001b[0;31m                 \u001b[0mrepo_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morganization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morganization\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprivate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprivate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_auth_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_auth_token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1009\u001b[0m             )\n\u001b[1;32m   1010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36m_get_repo_url_from_name\u001b[0;34m(repo_name, organization, private, use_auth_token)\u001b[0m\n\u001b[1;32m    982\u001b[0m             \u001b[0mprivate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprivate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m             \u001b[0mrepo_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 984\u001b[0;31m             \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    985\u001b[0m         )\n\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0mvalidate_repo_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/huggingface_hub/utils/_deprecation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     43\u001b[0m             )\n\u001b[1;32m     44\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/huggingface_hub/hf_api.py\u001b[0m in \u001b[0;36mcreate_repo\u001b[0;34m(self, repo_id, token, organization, private, repo_type, exist_ok, space_sdk, name)\u001b[0m\n\u001b[1;32m   1665\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1667\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1669\u001b[0m         \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/huggingface_hub/hf_api.py\u001b[0m in \u001b[0;36mcreate_repo\u001b[0;34m(self, repo_id, token, organization, private, repo_type, exist_ok, space_sdk, name)\u001b[0m\n\u001b[1;32m   1654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1655\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1656\u001b[0;31m             \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1657\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1658\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexist_ok\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m409\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/huggingface_hub/utils/_errors.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    240\u001b[0m                 \u001b[0;34m+\u001b[0m \u001b[0;34m\"\\nIf the repo is private, make sure you are authenticated.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m             )\n\u001b[0;32m--> 242\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRepositoryNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-638f383e-42e51d0f30ccd114386545a8)\n\nRepository Not Found for url: https://huggingface.co/api/repos/create.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf the repo is private, make sure you are authenticated.\nUnauthorized - Unauthorized"],"ename":"RepositoryNotFoundError","evalue":"401 Client Error. (Request ID: Root=1-638f383e-42e51d0f30ccd114386545a8)\n\nRepository Not Found for url: https://huggingface.co/api/repos/create.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf the repo is private, make sure you are authenticated.\nUnauthorized - Unauthorized","output_type":"error"}]},{"cell_type":"markdown","source":"# Trying to train another model **(extra!)**","metadata":{}},{"cell_type":"code","source":"# The percentage of the dataset you want to split as train and test\nTRAIN_TEST_SPLIT = 0.1\n\nMAX_INPUT_LENGTH = 1024  # Maximum length of the input to the model\nMIN_TARGET_LENGTH = 5  # Minimum length of the output by the model\nMAX_TARGET_LENGTH = 128  # Maximum length of the output by the model\nBATCH_SIZE = 8  # Batch-size for training our model\nLEARNING_RATE = 2e-5  # Learning-rate for training our model\nMAX_EPOCHS = 3  # Maximum number of epochs we will train the model for\n\n# This notebook is built on the t5-small checkpoint from the Hugging Face Model Hub\nMODEL_CHECKPOINT = \"t5-small\"","metadata":{"execution":{"iopub.status.busy":"2022-12-06T15:50:45.081288Z","iopub.execute_input":"2022-12-06T15:50:45.082106Z","iopub.status.idle":"2022-12-06T15:50:45.087463Z","shell.execute_reply.started":"2022-12-06T15:50:45.082056Z","shell.execute_reply":"2022-12-06T15:50:45.086284Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"if MODEL_CHECKPOINT in [\"t5-small\", \"t5-base\", \"t5-large\", \"t5-3b\", \"t5-11b\"]:\n    prefix = \"summarize: \"\nelse:\n    prefix = \"\"","metadata":{"execution":{"iopub.status.busy":"2022-12-06T15:50:46.615846Z","iopub.execute_input":"2022-12-06T15:50:46.616247Z","iopub.status.idle":"2022-12-06T15:50:46.622488Z","shell.execute_reply.started":"2022-12-06T15:50:46.616212Z","shell.execute_reply":"2022-12-06T15:50:46.621216Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def preprocess_function(examples):\n    inputs = [prefix + doc for doc in examples[\"maintext\"]]\n    model_inputs = tokenizer(inputs, max_length=MAX_INPUT_LENGTH, truncation=True)\n\n    # Setup the tokenizer for targets\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(\n            examples[\"title\"], max_length=MAX_TARGET_LENGTH, truncation=True\n        )\n\n    model_inputs[\"labels\"] = labels[\"input_ids\"]\n\n    return model_inputs","metadata":{"execution":{"iopub.status.busy":"2022-12-06T15:50:48.099242Z","iopub.execute_input":"2022-12-06T15:50:48.099623Z","iopub.status.idle":"2022-12-06T15:50:48.106359Z","shell.execute_reply.started":"2022-12-06T15:50:48.099593Z","shell.execute_reply":"2022-12-06T15:50:48.105261Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_CHECKPOINT, model_max_length=512)","metadata":{"execution":{"iopub.status.busy":"2022-12-06T15:50:49.799928Z","iopub.execute_input":"2022-12-06T15:50:49.800298Z","iopub.status.idle":"2022-12-06T15:50:51.608468Z","shell.execute_reply.started":"2022-12-06T15:50:49.800268Z","shell.execute_reply":"2022-12-06T15:50:51.607454Z"},"trusted":true},"execution_count":22,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30551408decb414b8063a6cde49428d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/773k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2c5a5865382844b788dd14e2c83a9868"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.32M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"561eb6f19571448b957b4d9c6140266b"}},"metadata":{}}]},{"cell_type":"code","source":"from datasets import load_dataset\n\nraw_datasets = load_dataset('csv', data_files={'train':'/kaggle/input/nyt-news/nyt_train.csv','validation':'/kaggle/input/nyt-news/nyt_val.csv', 'test':'/kaggle/input/nyt-news/nyt_test.csv'},delimiter='~')","metadata":{"execution":{"iopub.status.busy":"2022-12-06T15:50:53.262020Z","iopub.execute_input":"2022-12-06T15:50:53.262412Z","iopub.status.idle":"2022-12-06T15:50:53.434998Z","shell.execute_reply.started":"2022-12-06T15:50:53.262379Z","shell.execute_reply":"2022-12-06T15:50:53.434045Z"},"trusted":true},"execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"260c7eb198d94a968646639392999ca1"}},"metadata":{}}]},{"cell_type":"code","source":"tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)","metadata":{"execution":{"iopub.status.busy":"2022-12-06T15:50:56.920567Z","iopub.execute_input":"2022-12-06T15:50:56.921524Z","iopub.status.idle":"2022-12-06T15:51:13.386575Z","shell.execute_reply.started":"2022-12-06T15:50:56.921471Z","shell.execute_reply":"2022-12-06T15:51:13.385641Z"},"trusted":true},"execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/6 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53fbd226b9df40af969d98b934bb6893"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5488c3a70dbc45aea372ca14904307e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51c019d33d41487aaf92fddad0c916ce"}},"metadata":{}}]},{"cell_type":"code","source":"from transformers import TFAutoModelForSeq2SeqLM, DataCollatorForSeq2Seq\n\nmodel = TFAutoModelForSeq2SeqLM.from_pretrained(MODEL_CHECKPOINT)","metadata":{"execution":{"iopub.status.busy":"2022-12-06T15:51:15.675421Z","iopub.execute_input":"2022-12-06T15:51:15.676139Z","iopub.status.idle":"2022-12-06T15:51:27.285055Z","shell.execute_reply.started":"2022-12-06T15:51:15.676102Z","shell.execute_reply":"2022-12-06T15:51:27.284061Z"},"trusted":true},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/231M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eaf0ac4020d046da94591ef79ea9c2bf"}},"metadata":{}},{"name":"stderr","text":"2022-12-06 15:51:26.966674: W tensorflow/core/framework/cpu_allocator_impl.cc:80] Allocation of 65798144 exceeds 10% of free system memory.\nAll model checkpoint layers were used when initializing TFT5ForConditionalGeneration.\n\nAll the layers of TFT5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import DataCollatorForSeq2Seq\n\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model, return_tensors=\"tf\")","metadata":{"execution":{"iopub.status.busy":"2022-12-06T15:51:32.754267Z","iopub.execute_input":"2022-12-06T15:51:32.755075Z","iopub.status.idle":"2022-12-06T15:51:32.760378Z","shell.execute_reply.started":"2022-12-06T15:51:32.755023Z","shell.execute_reply":"2022-12-06T15:51:32.759393Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"train_dataset = tokenized_datasets[\"train\"].to_tf_dataset(\n    batch_size=BATCH_SIZE,\n    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n    shuffle=True,\n    collate_fn=data_collator,\n)\ntest_dataset = tokenized_datasets[\"test\"].to_tf_dataset(\n    batch_size=BATCH_SIZE,\n    columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n    shuffle=False,\n    collate_fn=data_collator,\n)\ngeneration_dataset = (\n    tokenized_datasets[\"test\"]\n    .shuffle()\n    .select(list(range(200)))\n    .to_tf_dataset(\n        batch_size=BATCH_SIZE,\n        columns=[\"input_ids\", \"attention_mask\", \"labels\"],\n        shuffle=False,\n        collate_fn=data_collator,\n    )\n)","metadata":{"execution":{"iopub.status.busy":"2022-12-06T15:51:35.064923Z","iopub.execute_input":"2022-12-06T15:51:35.065403Z","iopub.status.idle":"2022-12-06T15:51:35.180369Z","shell.execute_reply.started":"2022-12-06T15:51:35.065364Z","shell.execute_reply":"2022-12-06T15:51:35.179511Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\nmodel.compile(optimizer=optimizer)","metadata":{"execution":{"iopub.status.busy":"2022-12-06T15:51:37.352718Z","iopub.execute_input":"2022-12-06T15:51:37.353105Z","iopub.status.idle":"2022-12-06T15:51:37.367743Z","shell.execute_reply.started":"2022-12-06T15:51:37.353070Z","shell.execute_reply":"2022-12-06T15:51:37.366584Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stderr","text":"No loss specified in compile() - the model's internal loss computation will be used as the loss. Don't panic - this is a common way to train TensorFlow models in Transformers! To disable this behaviour please pass a loss argument, or explicitly pass `loss=None` if you do not want your model to compute a loss.\n","output_type":"stream"}]},{"cell_type":"code","source":"import keras_nlp\n\nrouge_l = keras_nlp.metrics.RougeL()\n\n\ndef metric_fn(eval_predictions):\n    predictions, labels = eval_predictions\n    decoded_predictions = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    for label in labels:\n        label[label < 0] = tokenizer.pad_token_id  # Replace masked label tokens\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    result = rouge_l(decoded_labels, decoded_predictions)\n    # We will print only the F1 score, you can use other aggregation metrics as well\n    result = {\"RougeL\": result[\"f1_score\"]}\n\n    return result","metadata":{"execution":{"iopub.status.busy":"2022-12-06T15:51:38.972128Z","iopub.execute_input":"2022-12-06T15:51:38.972559Z","iopub.status.idle":"2022-12-06T15:51:38.985726Z","shell.execute_reply.started":"2022-12-06T15:51:38.972522Z","shell.execute_reply":"2022-12-06T15:51:38.984622Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"from transformers.keras_callbacks import KerasMetricCallback\n\nmetric_callback = KerasMetricCallback(\n    metric_fn, eval_dataset=generation_dataset, predict_with_generate=True\n)\n\ncallbacks = [metric_callback]\n\n# For now we will use our test set as our validation_data\nmodel.fit(\n    train_dataset, validation_data=test_dataset, epochs=MAX_EPOCHS, callbacks=callbacks\n)","metadata":{"execution":{"iopub.status.busy":"2022-12-06T15:51:40.691322Z","iopub.execute_input":"2022-12-06T15:51:40.692040Z","iopub.status.idle":"2022-12-06T15:56:58.499859Z","shell.execute_reply.started":"2022-12-06T15:51:40.692004Z","shell.execute_reply":"2022-12-06T15:56:58.497676Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Epoch 1/3\n750/750 [==============================] - 280s 354ms/step - loss: 3.8973 - val_loss: 3.2662\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOverflowError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_22/2959997578.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# For now we will use our test set as our validation_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m model.fit(\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m )\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1228\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1230\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1231\u001b[0m         \u001b[0mtraining_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1232\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    411\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m       \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/keras_callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0mall_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_postprocess_predictions_or_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0mmetric_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetric_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_preds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m             raise TypeError(\n","\u001b[0;32m/tmp/ipykernel_22/3460990998.py\u001b[0m in \u001b[0;36mmetric_fn\u001b[0;34m(eval_predictions)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmetric_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mdecoded_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mlabel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token_id\u001b[0m  \u001b[0;31m# Replace masked label tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mbatch_decode\u001b[0;34m(self, sequences, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3311\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3312\u001b[0m             )\n\u001b[0;32m-> 3313\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3314\u001b[0m         ]\n\u001b[1;32m   3315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   3311\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3312\u001b[0m             )\n\u001b[0;32m-> 3313\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3314\u001b[0m         ]\n\u001b[1;32m   3315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3347\u001b[0m             \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3348\u001b[0m             \u001b[0mclean_up_tokenization_spaces\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclean_up_tokenization_spaces\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3349\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3350\u001b[0m         )\n\u001b[1;32m   3351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m    546\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m             \u001b[0mtoken_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclean_up_tokenization_spaces\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOverflowError\u001b[0m: out of range integral type conversion attempted"],"ename":"OverflowError","evalue":"out of range integral type conversion attempted","output_type":"error"}]},{"cell_type":"code","source":"from transformers import pipeline\n\nsummarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer, framework=\"tf\")\n\nsummarizer(\n    raw_datasets[\"test\"][0][\"maintext\"],\n    min_length=MIN_TARGET_LENGTH,\n    max_length=MAX_TARGET_LENGTH,\n)","metadata":{},"execution_count":null,"outputs":[]}]}