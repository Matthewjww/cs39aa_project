{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'nyt_train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [61], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnlp\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m load_dataset\n\u001B[0;32m----> 2\u001B[0m dataset \u001B[38;5;241m=\u001B[39m \u001B[43mload_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mcsv\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata_files\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mnyt_train.csv\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msplit\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtrain\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m dataset_val \u001B[38;5;241m=\u001B[39m load_dataset(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcsv\u001B[39m\u001B[38;5;124m'\u001B[39m, data_files\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnyt_train.csv\u001B[39m\u001B[38;5;124m'\u001B[39m, split\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mvalidation\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(dataset\u001B[38;5;241m.\u001B[39mkeys())\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/nlp/load.py:536\u001B[0m, in \u001B[0;36mload_dataset\u001B[0;34m(path, name, version, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, save_infos, **config_kwargs)\u001B[0m\n\u001B[1;32m    533\u001B[0m builder_cls \u001B[38;5;241m=\u001B[39m import_main_class(module_path, dataset\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m    535\u001B[0m \u001B[38;5;66;03m# Instantiate the dataset builder\u001B[39;00m\n\u001B[0;32m--> 536\u001B[0m builder_instance: DatasetBuilder \u001B[38;5;241m=\u001B[39m \u001B[43mbuilder_cls\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    537\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcache_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcache_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    538\u001B[0m \u001B[43m    \u001B[49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    539\u001B[0m \u001B[43m    \u001B[49m\u001B[43mversion\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mversion\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    540\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_dir\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    541\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata_files\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_files\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    542\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mhash\u001B[39;49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mhash\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    543\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfeatures\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfeatures\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    544\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mconfig_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    545\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    547\u001B[0m \u001B[38;5;66;03m# Download and prepare data\u001B[39;00m\n\u001B[1;32m    548\u001B[0m builder_instance\u001B[38;5;241m.\u001B[39mdownload_and_prepare(\n\u001B[1;32m    549\u001B[0m     download_config\u001B[38;5;241m=\u001B[39mdownload_config, download_mode\u001B[38;5;241m=\u001B[39mdownload_mode, ignore_verifications\u001B[38;5;241m=\u001B[39mignore_verifications,\n\u001B[1;32m    550\u001B[0m )\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/nlp/builder.py:150\u001B[0m, in \u001B[0;36mDatasetBuilder.__init__\u001B[0;34m(self, cache_dir, name, hash, features, **config_kwargs)\u001B[0m\n\u001B[1;32m    148\u001B[0m \u001B[38;5;66;03m# Prepare config: DatasetConfig contains name, version and description but can be extended by each dataset\u001B[39;00m\n\u001B[1;32m    149\u001B[0m config_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m((key, value) \u001B[38;5;28;01mfor\u001B[39;00m key, value \u001B[38;5;129;01min\u001B[39;00m config_kwargs\u001B[38;5;241m.\u001B[39mitems() \u001B[38;5;28;01mif\u001B[39;00m value \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m--> 150\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_create_builder_config\u001B[49m\u001B[43m(\u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mconfig_kwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    152\u001B[0m \u001B[38;5;66;03m# prepare info: DatasetInfo are a standardized dataclass across all datasets\u001B[39;00m\n\u001B[1;32m    153\u001B[0m \u001B[38;5;66;03m# Prefill datasetinfo\u001B[39;00m\n\u001B[1;32m    154\u001B[0m info \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_exported_dataset_info()\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/nlp/builder.py:261\u001B[0m, in \u001B[0;36mDatasetBuilder._create_builder_config\u001B[0;34m(self, name, **config_kwargs)\u001B[0m\n\u001B[1;32m    259\u001B[0m m\u001B[38;5;241m.\u001B[39mupdate(key\u001B[38;5;241m.\u001B[39mencode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m    260\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m data_file \u001B[38;5;129;01min\u001B[39;00m data_files[key]:\n\u001B[0;32m--> 261\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mdata_file\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[1;32m    262\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28miter\u001B[39m(\u001B[38;5;28;01mlambda\u001B[39;00m: f\u001B[38;5;241m.\u001B[39mread(\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m<<\u001B[39m \u001B[38;5;241m20\u001B[39m), \u001B[38;5;124mb\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m    263\u001B[0m             m\u001B[38;5;241m.\u001B[39mupdate(chunk)\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'nyt_train.csv'"
     ]
    }
   ],
   "source": [
    "from nlp import load_dataset\n",
    "dataset = load_dataset('csv', data_files='nyt_train.csv', split='train')\n",
    "dataset_val = load_dataset('csv', data_files='nyt_train.csv', split='validation')\n",
    "print(dataset.keys())\n",
    "print(\"Size of train dataset: \", dataset['train'].shape)\n",
    "print(\"Size of Validation dataset: \", dataset_val['validation'].shape)\n",
    "## Look at Sample Examples\n",
    "print(dataset['train'][0].keys())\n",
    "print(\" Example of text: \", dataset['train'][0]['maintext'])\n",
    "print(\" Example of Summary: \", dataset['train'][0]['title'])\n",
    "## Estimate Average Length of Text and Summary\n",
    "tiny_dataset = dataset['train'].select(list(range(0, 100)))\n",
    "text_len = []\n",
    "summary_len=[]\n",
    "for i in range(len(tiny_dataset)):\n",
    "    example = tiny_dataset[i]\n",
    "    text_example = example['maintext']\n",
    "    text_example = text_example.replace('\\n','')\n",
    "    text_words = text_example.split()\n",
    "    text_len.append(len(text_words))\n",
    "    summary_example = example['title']\n",
    "    summary_example = summary_example.replace('\\n','')\n",
    "    summary_words = summary_example.split()\n",
    "    summary_len.append(len(summary_words))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.hist(text_len)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('nyt_train.csv')\n",
    "df.drop(columns=['Unnamed: 0'])\n",
    "df.to_csv('nyt_train_3.csv', sep='~', index=False)\n",
    "df2 = pd.read_csv('nyt_val.csv')\n",
    "df2.drop(columns=['Unnamed: 0'])\n",
    "df.to_csv('nyt_val_3.csv', sep='~', index=False)\n",
    "df2.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob, os, json, csv\n",
    "\n",
    "\n",
    "json_dir = 'nytimes.com/'\n",
    "\n",
    "json_pattern = os.path.join(json_dir, '*.json')\n",
    "file_list = glob.glob(json_pattern)\n",
    "\n",
    "dfs = []\n",
    "for file in file_list:\n",
    "    with open(file) as f:\n",
    "        json_data = pd.json_normalize(json.loads(f.read()))\n",
    "        json_data['site'] = file.rsplit(\"/\", 1)[-1]\n",
    "    dfs.append(json_data)\n",
    "df = pd.concat(dfs)\n",
    "\n",
    "df = df.dropna(subset =['title', 'maintext']) # remove entries where the title or maintext are missing.\n",
    "df = df.drop(columns=['date_download','date_modify','date_publish','authors','filename','image_url','localpath','title_rss','source_domain','url','site','title_page', 'language', 'description'])\n",
    "df.drop_duplicates('maintext')\n",
    "df['maintext'] = df['maintext'].str.replace('/n',' ')\n",
    "df['maintext'] = df['maintext'].str.replace('~',' ')\n",
    "df['title'] = df['title'].str.replace('/n',' ')\n",
    "df['title'] = df['title'].str.replace('~',' ')\n",
    "df['maintext'] = df['maintext'].str.replace('/r',' ')\n",
    "df['title'] = df['title'].str.replace('/r',' ')\n",
    "df['maintext'] = df['maintext'].str.strip()\n",
    "df['title'] = df['title'].str.strip()\n",
    "df['maintext'] = df['maintext'].str.split()\n",
    "df['title'] = df['title'].str.split()\n",
    "df['maintext'] = df['maintext'].str.join(sep=' ')\n",
    "df['title'] = df['title'].str.join(sep=' ')\n",
    "df.to_csv('nyt_all.csv', sep='~')\n",
    "#df.to_csv('nyt_val1.csv', sep='~')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [],
   "source": [
    "df_train = df.iloc[:6000]\n",
    "df_val = df.iloc[6001:9001]\n",
    "\n",
    "df_train.to_csv('nyt_train.csv', sep='~', encoding='utf-8')\n",
    "df_val.to_csv('nyt_val.csv', sep='~', encoding='utf-8')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "outputs": [],
   "source": [
    "df_test = df.iloc[9002:10002]\n",
    "df_test.to_csv('nyt_test.csv', sep='~', encoding='utf-8')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}